# Semseg

[![Go Report Card](https://goreportcard.com/badge/github.com/cmsdko/semseg)](https://goreportcard.com/report/github.com/cmsdko/semseg)
[![Go.Dev Reference](https://img.shields.io/badge/go.dev-reference-blue?logo=go&logoColor=white)](https://pkg.go.dev/github.com/cmsdko/semseg)
[![Build Status](https://github.com/cmsdko/semseg/actions/workflows/go.yml/badge.svg)](https://github.com/cmsdko/semseg/actions/workflows/go.yml)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**Semseg** is a lightweight, zero-dependency Go library for splitting text into semantically coherent chunks.  
It supports multi-language stopword removal, stemming, and abbreviation normalization â€” all in pure Go.  
Perfect for preprocessing text in RAG (Retrieval-Augmented Generation) pipelines, summarization, or any NLP tasks.

## Features

- **Semantic Splitting**: Splits at points of low semantic similarity while keeping related content together.
- **Strict Token Limit**: Ensures no chunk exceeds `MaxTokens` (unless a single sentence is larger).
- **Language Awareness**: Automatic or manual language detection, stopword removal, stemming, and abbreviation handling.
- **Configurable**: Fineâ€‘tune similarity thresholds, detection modes, and preprocessing options.
- **Zero Dependencies**: 100% Go, no external models or libraries.
- **Fast and Lightweight**: Classic TFâ€‘IDF approach, optimized for CPU workloads.

## Installation

```sh
go get github.com/cmsdko/semseg
```

## Quick Start

```go
package main

import (
	"fmt"
	"log"

	"github.com/cmsdko/semseg"
)

func main() {
	text := `The solar system consists of the Sun and the planets. A rocket journey to other planets takes a long time. The ocean covers most of the Earth's surface. Amazing creatures live in the depths of the ocean.`

	opts := semseg.Options{
		MaxTokens: 15,
	}

	chunks, err := semseg.Segment(text, opts)
	if err != nil {
		log.Fatal(err)
	}

	for i, c := range chunks {
		fmt.Printf("--- Chunk %d (%d tokens) ---\n%s\n\n", i+1, c.NumTokens, c.Text)
	}
}
```

### Expected Output

```
--- Chunk 1 (14 tokens) ---
The solar system consists of the Sun and the planets. A rocket journey to other planets takes a long time.

--- Chunk 2 (13 tokens) ---
The ocean covers most of the Earth's surface. Amazing creatures live in the depths of the ocean.
```

## How It Works

1. **Sentence Splitting** â†’ text is divided into sentences (multiâ€‘language aware).
2. **Normalization** â†’ abbreviations like `U.S.A.` or `Ñ‚.Ðµ.` are normalized before splitting.
3. **Stopword Removal & Stemming** â†’ optional preprocessing to reduce noise.
4. **Vectorization** â†’ each sentence is turned into a TFâ€‘IDF vector.
5. **Cohesion Scoring** â†’ cosine similarity between adjacent sentences is calculated.
6. **Boundary Detection** â†’ splits occur at local minima or below thresholds.
7. **Chunk Assembly** â†’ sentences grouped into chunks respecting `MaxTokens`.

## Processing Pipeline (based on Options)

Depending on `Options`, the pipeline adapts as follows:

- **Language Detection**
    - `Language` set â†’ skip detection, force specific language.
    - `LanguageDetectionTokens > 0` â†’ detect language from first *N* tokens (slower, but enables use of JSON-based contractions/stopwords).
    - `LanguageDetectionMode` â†’ choose detection strategy (`first_sentence`, `first_ten_sentences`, `per_sentence`, `full_text`).
    - âš¡ For **performance**, prefer `first_sentence` or `full_text`.
    - ðŸ§© For **flexibility**, use token-based detection â€” it allows leveraging custom stopwords and abbreviations.

- **Abbreviation Normalization**
    - Controlled by `PreNormalizeAbbreviations`.
    - Removes dots in known contractions and acronyms (configurable in JSON).

- **Stopword Removal**
    - Controlled by `EnableStopWordRemoval`.
    - Uses stopwords from `internal/data/stopwords.json`.
    - You can **add/remove languages or stopwords** by editing this JSON.

- **Stemming**
    - Controlled by `EnableStemming`.
    - Uses simple affix-based rules per language, defined in JSON.

- **Chunk Assembly**
    - Always respects `MaxTokens`.
    - Splits on semantic boundaries or when exceeding the limit.

ðŸ‘‰ The `stopwords.json` file is intentionally **user-editable**: you can remove or add words and even define new languages with custom rules. This makes the library flexible without depending on external NLP libraries.

## API Example

You can run Semseg as an HTTP API using the provided `example` project:

```sh
docker-compose -f example/docker-compose.yml up --build
```

Then send a request:

```sh
curl -X POST http://localhost:8080/segment -d '{
  "text": "Mars is red. Venus is hot. The ocean is blue.",
  "max_tokens": 10
}' -H "Content-Type: application/json"
```

Response:

```json
[
  {"Text":"Mars is red.","Sentences":["Mars is red."],"NumTokens":3},
  {"Text":"Venus is hot.","Sentences":["Venus is hot."],"NumTokens":3},
  {"Text":"The ocean is blue.","Sentences":["The ocean is blue."],"NumTokens":4}
]
```
## Known Limitations

- **Chinese and other CJK languages**:  
  The library does not implement word segmentation for Han/Hiragana/Katakana/Hangul scripts.  
  For these languages, stopword removal and stemming are not applied, and language detection will usually return `unknown`.  
  Result: text is still split into sentences, but semantic cohesion may be poor.

- **Language limit (64 max)**:  
  Internally, the library uses a `uint64` bitmask to optimize stopword lookups.  
  This means no more than 64 languages can be supported at once.  
  If `stopwords.json` defines more than 64, initialization will fail with a fatal error.

## Contributing

Contributions are welcome! Open issues or PRs to suggest features, report bugs, or improve language resources.

## License

MIT License. See [LICENSE](LICENSE).
